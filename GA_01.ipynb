{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAuKtIHZxsBX"
      },
      "outputs": [],
      "source": [
        "# 0) Setup\n",
        "\n",
        "!pip -q install transformers datasets accelerate evaluate\n",
        "\n",
        "import os, torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"gpt2\"  # use \"distilgpt2\" if you're short on GPU RAM\n",
        "OUTPUT_DIR = \"gpt2-codecraft-ga01\"\n",
        "\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Upload dataset (data.txt)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload data.txt\n",
        "\n",
        "# Ensure filename is data.txt\n",
        "assert \"data.txt\" in uploaded, \"Please upload a file named data.txt\"\n"
      ],
      "metadata": {
        "id": "bQbSo-exz6Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deduplicate lines in data.txt (simple but effective)\n",
        "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = [ln.rstrip() for ln in f.readlines()]\n",
        "\n",
        "seen = set()\n",
        "cleaned = []\n",
        "for ln in lines:\n",
        "    key = ln.strip()\n",
        "    if key and key not in seen:\n",
        "        seen.add(key)\n",
        "        cleaned.append(ln)\n",
        "\n",
        "with open(\"data_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(cleaned))\n",
        "\n",
        "print(\"Original lines:\", len(lines))\n",
        "print(\"Cleaned lines:\", len(cleaned))\n"
      ],
      "metadata": {
        "id": "caBs6Ai4i26f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Load dataset as HF dataset\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"data_clean.txt\"})\n",
        "print(dataset)\n",
        "print(\"Sample:\\n\", dataset[\"train\"][0][\"text\"][:300])\n"
      ],
      "metadata": {
        "id": "hGEDW2-t0KTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Tokenizer + Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "y6J2MZyI0QUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Tokenize + chunk into blocks\n",
        "\n",
        "BLOCK_SIZE = 256  # 128/256 works well on Colab\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    # concatenate then split into BLOCK_SIZE\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_len = len(concatenated[\"input_ids\"])\n",
        "    total_len = (total_len // BLOCK_SIZE) * BLOCK_SIZE\n",
        "    result = {\n",
        "        k: [t[i:i+BLOCK_SIZE] for i in range(0, total_len, BLOCK_SIZE)]\n",
        "        for k, t in concatenated.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_dataset = tokenized.map(group_texts, batched=True)\n",
        "lm_dataset\n"
      ],
      "metadata": {
        "id": "vYwDgVxL0Vps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Training\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,   # effective batch size = 16\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"no\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset[\"train\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "SDAZLuqp0ZXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Save model\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "Pv_GiIxc0jJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GenerationConfig\n",
        "\n",
        "def build_generator(model_path_or_name):\n",
        "    gen = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_path_or_name,\n",
        "        tokenizer=model_path_or_name,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    return gen\n",
        "\n",
        "def generate(gen, prompt, max_new_tokens=120):\n",
        "    return gen(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.15,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=50256\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "prompt = \"Topic: Transformers\\nWrite a concise note in 5-7 lines:\\n\"\n",
        "\n",
        "base_gen = build_generator(MODEL_NAME)\n",
        "ft_gen = build_generator(OUTPUT_DIR)\n",
        "\n",
        "print(\"=== BASE GPT-2 ===\")\n",
        "print(generate(base_gen, prompt))\n",
        "\n",
        "print(\"\\n=== FINE-TUNED GPT-2 ===\")\n",
        "print(generate(ft_gen, prompt))\n"
      ],
      "metadata": {
        "id": "irJQqhvBj2KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Export artifacts to download / GitHub\n",
        "\n",
        "!zip -r CODECRAFT_GA_01_artifacts.zip {OUTPUT_DIR}\n",
        "print(\"Zipped artifacts.\")\n"
      ],
      "metadata": {
        "id": "NdXFxRkV0sO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "OUTPUT_DIR = \"gpt2-codecraft-ga01\"\n",
        "SUBMIT_DIR = \"CODECRAFT_GA_01_submit\"\n",
        "\n",
        "# clean old submit folder\n",
        "if os.path.exists(SUBMIT_DIR):\n",
        "    shutil.rmtree(SUBMIT_DIR)\n",
        "\n",
        "os.makedirs(SUBMIT_DIR, exist_ok=True)\n",
        "\n",
        "# Copy only final model + tokenizer files from OUTPUT_DIR (NOT checkpoints)\n",
        "keep_files = {\n",
        "    \"config.json\",\n",
        "    \"generation_config.json\",\n",
        "    \"model.safetensors\",\n",
        "    \"tokenizer.json\",\n",
        "    \"tokenizer_config.json\",\n",
        "    \"special_tokens_map.json\",\n",
        "    \"vocab.json\",\n",
        "    \"merges.txt\"\n",
        "}\n",
        "\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    src = os.path.join(OUTPUT_DIR, f)\n",
        "    if os.path.isfile(src) and f in keep_files:\n",
        "        shutil.copy2(src, os.path.join(SUBMIT_DIR, f))\n",
        "\n",
        "print(\"Submit folder files:\", os.listdir(SUBMIT_DIR))\n"
      ],
      "metadata": {
        "id": "QB7_u8rHNWac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r CODECRAFT_GA_01_submit.zip CODECRAFT_GA_01_submit"
      ],
      "metadata": {
        "id": "4B6Z4gyxNY6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}